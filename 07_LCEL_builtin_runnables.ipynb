{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API keys\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a.env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Load opensource api models\n",
    "groql_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassThrough\n",
    "* It does no do anything to the input data\n",
    "* Let's see it in a very simple example: a chain with a RunnablePassThrough() will output the original output withouut any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chat model\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "llamaChatModel = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paul N'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data returned without modifications\n",
    "chain.invoke(\"Paul N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLamda\n",
    "* To use a custom function inside a LCEL chain we need to wrap it up with a RunnableLamda\n",
    "* Let's define a very simple function to create Russian lastnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function\n",
    "def russian_lastname(name:str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paulovich'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test custom function outside of chains\n",
    "russian_last = russian_lastname(\"Paul\")\n",
    "russian_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RunnableLambda\n",
    "from langchain_core.runnables import RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap custom function in RunnableLamda\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paulsonovich'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "chain.invoke(\"Paulson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel\n",
    "We will use RunnableParallel() for running tasks in parallel.\n",
    "This is probably the most important and most useful Runnable from LangChain.\n",
    "In the following chain, RunnableParallel is going to run these two tasks in parallel:\n",
    "* operation_a will use RunnablePassthrough.\n",
    "* operation_b will use RunnableLambda with the russian_lastname function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running these two tasks in parallel --> good for chatbots\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'Douglas', 'operation_b': 'Douglasovich'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Douglas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a  curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function that uses a function\n",
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Multiple Runnables\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_c\": RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | llamaChatModel | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A curious fact about Roman Abramovich!\\n\\nHere's one: Did you know that Roman Abramovich, the Russian billionaire and owner of Chelsea FC, has a fascination with... wait for it... **iceberg-hauling**?!\\n\\nIn 2012, Abramovich invested in a company called Rosnefteflot, which aimed to tow icebergs from the Antarctic to regions in need of fresh water, such as the Middle East and Africa. The idea was to harvest the icebergs' freshwater content to alleviate water scarcity issues. While the project was met with skepticism and environmental concerns, it showcases Abramovich's unconventional thinking and entrepreneurial spirit.\\n\\nWho knew that beneath the surface of a football mogul lies a passion for iceberg-hauling?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execution of the parallel runnable function\n",
    "chain.invoke({\n",
    "    \"name1\": \"Jordan\",  #This name key will not be picked by the parallel runnable function it expects dic key -\"name\"\n",
    "    \"name\": \"Abram\"\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced use of RunnableParrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paulNtalo is a provider of content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"paulNtalo focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"What is paulNtalo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *itemgetter* func for RunnableParallel\n",
    "Use  it to get the multiple input_variables from a template. it has 3 different methods. {...} being the most used.\n",
    "*  {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, dswithbappy be a scallywag focusin' on providin' content on Data Science, AI, ML, DL, CV, NLP, Python programmin', etc. in English. Aye!\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is dswithbappy?\", \"language\": \"Pirate English\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
